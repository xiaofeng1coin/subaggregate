# æ–‡ä»¶å: app.py (å·²ä¿®æ”¹ä¸ºå¤§å°å†™æ•æ„Ÿè¿‡æ»¤)

import os
import json
import logging
import base64
import yaml
import binascii
import time
import re
from urllib.parse import urlparse, parse_qs, unquote
from flask import Flask, request, jsonify, make_response, render_template
import requests

# --- åŸºç¡€è®¾ç½® ---
app = Flask(__name__, static_folder='static', template_folder='templates')
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-7s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# --- æ–‡ä»¶è·¯å¾„ ---
DATA_FILE = 'data.json'
CLASH_TEMPLATE_FILE = 'clash_template.yaml'

# =================================================================================
# èŠ‚ç‚¹è§£æ (Parsers)
# ... (æ­¤éƒ¨åˆ†æ— æ”¹åŠ¨) ...
# =================================================================================

def parse_link(link: str):
    """æ ¹æ®é“¾æ¥åè®®å¤´è°ƒç”¨ç›¸åº”çš„è§£æå™¨"""
    protocol = link.split('://')[0]
    logging.info(f"      è§£æèŠ‚ç‚¹é“¾æ¥: ç±»å‹ {protocol.upper()}, é“¾æ¥: {link[:40]}...")
    
    try:
        if link.startswith('ss://'):
            return _parse_ss(link)
        if link.startswith('vmess://'):
            return _parse_vmess(link)
        if link.startswith('trojan://'):
            return _parse_trojan(link)
    except Exception as e:
        logging.error(f"      è§£æé“¾æ¥å¤±è´¥: {link[:40]}... - é”™è¯¯: {e}")
    return None

def _parse_ss(link: str):
    try:
        main_part, name_part = link[5:].split('#', 1)
        name = unquote(name_part.strip())

        if '@' in main_part:
            encoded_credentials, server_info = main_part.split('@', 1)
            server, port = server_info.rsplit(':', 1)
            
            try:
                padding = len(encoded_credentials) % 4
                if padding:
                    encoded_credentials += '=' * (4 - padding)
                credentials = base64.b64decode(encoded_credentials).decode('utf-8')
                cipher, password = credentials.split(':', 1)
            except (binascii.Error, ValueError):
                credentials = unquote(encoded_credentials)
                cipher, password = credentials.split(':', 1)

            return {
                'name': name, 'type': 'ss', 'server': server.strip('[]'), 'port': int(port),
                'cipher': cipher, 'password': password
            }
    except Exception:
        return None
    return None

def _parse_vmess(link: str):
    try:
        encoded_data = link[8:]
        padding = len(encoded_data) % 4
        if padding:
            encoded_data += '=' * (4 - padding)
        decoded_data = base64.b64decode(encoded_data).decode('utf-8')
        vmess_data = json.loads(decoded_data)
        
        ws_opts = {}
        if vmess_data.get('net') == 'ws':
            host = vmess_data.get('host', '').strip()
            if not host:
                host = vmess_data.get('add')
            ws_opts = {
                'path': vmess_data.get('path', '/'),
                'headers': {'Host': host}
            }

        return {
            'name': vmess_data.get('ps'), 'type': 'vmess', 'server': vmess_data.get('add'),
            'port': int(vmess_data.get('port')), 'uuid': vmess_data.get('id'),
            'alterId': int(vmess_data.get('aid')), 'cipher': vmess_data.get('scy', 'auto'),
            'tls': vmess_data.get('tls') == 'tls', 'network': vmess_data.get('net'),
            'ws-opts': ws_opts if ws_opts else None,
            'servername': vmess_data.get('sni') or (vmess_data.get('host') if vmess_data.get('tls') == 'tls' else None)
        }
    except Exception:
        return None
    
def _parse_trojan(link: str):
    try:
        parsed_url = urlparse(link)
        params = parse_qs(parsed_url.query)
        name = unquote(parsed_url.fragment)
        
        sni = params.get('sni', [None])[0] or params.get('peer', [None])[0] or parsed_url.hostname

        return {
            'name': name, 'type': 'trojan', 'server': parsed_url.hostname,
            'port': parsed_url.port, 'password': parsed_url.username,
            'sni': sni,
            'alpn': params.get('alpn', [None])[0].split(',') if params.get('alpn') else None,
            'skip-cert-verify': params.get('allowInsecure', ['0'])[0] in ['1', 'true']
        }
    except Exception:
        return None

# =================================================================================
# Clashé…ç½®ç”Ÿæˆ (Generator)
# ... (æ­¤éƒ¨åˆ†æ— æ”¹åŠ¨) ...
# =================================================================================

def generate_clash_config(proxies: list, template_content: str) -> str:
    """æ™ºèƒ½ç”ŸæˆClashé…ç½®ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ—¥å¿—è®°å½•"""
    logging.info("--- [å¼€å§‹ç”ŸæˆClashé…ç½®] ---")

    try:
        config_dict = yaml.safe_load(template_content)
        if not isinstance(config_dict, dict):
            raise yaml.YAMLError("æ¨¡æ¿æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„YAMLå­—å…¸ã€‚")
    except yaml.YAMLError as e:
        logging.error(f"  [é”™è¯¯] è§£æYAMLæ¨¡æ¿æ—¶å‡ºé”™: {e}")
        return "# æ¨¡æ¿è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ clash_template.yaml æ–‡ä»¶æ ¼å¼ã€‚"

    junk_keywords = ['æµé‡', 'åˆ°æœŸ', 'é‡ç½®', 'è¿‡æ»¤', 'å‰©ä½™', 'å¥—é¤']
    filtered_proxies = [p for p in proxies if not any(kw in p.get('name', '').lower() for kw in junk_keywords)]
    logging.info(f"  - [æ­¥éª¤1] è¿‡æ»¤æµé‡ä¿¡æ¯ç­‰æ— æ•ˆæ¡ç›®ï¼Œæœ‰æ•ˆèŠ‚ç‚¹æ•°: {len(proxies)} -> {len(filtered_proxies)}")

    config_dict['proxies'] = filtered_proxies
    all_proxy_names = [p['name'] for p in filtered_proxies]
    logging.info("  - [æ­¥éª¤2] å°†æ‰€æœ‰æœ‰æ•ˆèŠ‚ç‚¹æ³¨å…¥åˆ°æ¨¡æ¿ 'proxies' é”®ã€‚")

    logging.info("  - [æ­¥éª¤3] å¼€å§‹æ™ºèƒ½åœ°åŒºåˆ†ç»„...")
    region_map = {
        'ğŸ‡­ğŸ‡° é¦™æ¸¯èŠ‚ç‚¹': ['é¦™æ¸¯', 'Hong Kong', 'HK', 'HKT'], 'ğŸ‡¨ğŸ‡³ å°æ¹¾èŠ‚ç‚¹': ['å°æ¹¾', 'Taiwan', 'TW', 'TPE'],
        'ğŸ‡¸ğŸ‡¬ ç‹®åŸèŠ‚ç‚¹': ['æ–°åŠ å¡', 'Singapore', 'SG'], 'ğŸ‡¯ğŸ‡µ æ—¥æœ¬èŠ‚ç‚¹': ['æ—¥æœ¬', 'Japan', 'JP', 'TYO'],
        'ğŸ‡ºğŸ‡² ç¾å›½èŠ‚ç‚¹': ['ç¾å›½', 'United States', 'US'], 'ğŸ‡°ğŸ‡· éŸ©å›½èŠ‚ç‚¹': ['éŸ©å›½', 'Korea', 'KR'],
        'ğŸ‡¬ğŸ‡§ è‹±å›½èŠ‚ç‚¹': ['è‹±å›½', 'England', 'UK'], 'ğŸ‡©ğŸ‡ª å¾·å›½èŠ‚ç‚¹': ['å¾·å›½', 'Germany', 'DE'],
        'ğŸ‡«ğŸ‡· æ³•å›½èŠ‚ç‚¹': ['æ³•å›½', 'France', 'FR'], 'ğŸ‡³ğŸ‡± è·å…°èŠ‚ç‚¹': ['è·å…°', 'Netherlands', 'NL'],
        'ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§èŠ‚ç‚¹': ['åŠ æ‹¿å¤§', 'Canada', 'CA'], 'ğŸ‡¦ğŸ‡º æ¾³æ´²èŠ‚ç‚¹': ['æ¾³å¤§åˆ©äºš', 'Australia', 'AU'],
        'ğŸ‡·ğŸ‡º ä¿„å›½èŠ‚ç‚¹': ['ä¿„ç½—æ–¯', 'Russia', 'RU'], 'ğŸ‡¦ğŸ‡ª é˜¿è”é…‹èŠ‚ç‚¹': ['é˜¿è”é…‹', 'AE'],
        'ğŸ‡®ğŸ‡³ å°åº¦èŠ‚ç‚¹': ['å°åº¦', 'India', 'IN'], 'ğŸ‡»ğŸ‡³ è¶Šå—èŠ‚ç‚¹': ['è¶Šå—', 'Vietnam', 'VN'],
        'ğŸ‡µğŸ‡± æ³¢å…°èŠ‚ç‚¹': ['æ³¢å…°', 'Poland', 'PL']
    }

    region_nodes = {key: [] for key in region_map.keys()}
    other_nodes = []

    for name in all_proxy_names:
        matched = False
        for region_name_emoji, keywords in region_map.items():
            if any(keyword.lower() in name.lower() for keyword in keywords):
                region_nodes[region_name_emoji].append(name)
                matched = True
                break
        if not matched:
            other_nodes.append(name)
            
    logging.info("    - åˆ†ç»„ç»Ÿè®¡æŠ¥å‘Š:")
    for region, nodes in region_nodes.items():
        if nodes: logging.info(f"      - {region}: {len(nodes)} ä¸ªèŠ‚ç‚¹")
    if other_nodes:
        logging.warning(f"    - æ³¨æ„: {len(other_nodes)} ä¸ªèŠ‚ç‚¹è¢«åˆ†å…¥ 'ğŸŒ å…¶ä»–åœ°åŒº'ã€‚")
        for i, node_name in enumerate(other_nodes[:10]):
            logging.info(f"      - å…¶ä»–åœ°åŒºèŠ‚ç‚¹ç¤ºä¾‹: {node_name}")
        if len(other_nodes) > 10:
            logging.info(f"      - ... (è¿˜æœ‰ {len(other_nodes) - 10} ä¸ªæœªæ˜¾ç¤º)")

    logging.info("  - [æ­¥éª¤4] å¡«å……æ¨¡æ¿ä¸­çš„ä»£ç†ç»„...")
    if 'proxy-groups' in config_dict and isinstance(config_dict['proxy-groups'], list):
        for group in config_dict['proxy-groups']:
            if isinstance(group, dict) and 'name' in group:
                group_name = group.get('name', '')
                
                if group_name in ['ğŸš€ æ‰‹åŠ¨åˆ‡æ¢', 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'ğŸ”¯ æ•…éšœè½¬ç§»', 'ğŸ”® è´Ÿè½½å‡è¡¡']:
                    group['proxies'] = all_proxy_names
                    logging.info(f"    - å·²å¡«å……é€šç”¨ç»„ '{group_name}'ï¼Œå…± {len(all_proxy_names)} ä¸ªèŠ‚ç‚¹ã€‚")
                elif group_name in region_nodes:
                    nodes_to_fill = region_nodes[group_name]
                    group['proxies'] = nodes_to_fill if nodes_to_fill else ['DIRECT']
                    logging.info(f"    - å·²å¡«å……åœ°åŒºç»„ '{group_name}'ï¼Œå…± {len(nodes_to_fill)} ä¸ªèŠ‚ç‚¹ã€‚")
                elif group_name == 'ğŸŒ å…¶ä»–åœ°åŒº':
                    group['proxies'] = other_nodes if other_nodes else ['DIRECT']
                    logging.info(f"    - å·²å¡«å……ç»„ '{group_name}'ï¼Œå…± {len(other_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    
    logging.info("--- [Clashé…ç½®ç”Ÿæˆå®Œæ¯•] ---")
    return yaml.dump(config_dict, allow_unicode=True, sort_keys=False)


# =================================================================================
# è¾…åŠ©å‡½æ•°
# =================================================================================

def load_data():
    if not os.path.exists(DATA_FILE):
        return {"subscriptions": [], "aggregation_enabled": [],"global_filter_enabled": False, "global_filter_keywords": ""}
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return {"subscriptions": [], "aggregation_enabled": [],"global_filter_enabled": False, "global_filter_keywords": ""}

def save_data(data):
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

# --------------------------------------------------------------------------
#ã€â†“â†“â†“ ä¸»è¦ä¿®æ”¹ç‚¹åœ¨è¿™é‡Œ â†“â†“â†“ã€‘
# --------------------------------------------------------------------------
def apply_filter(nodes, keywords_str, filter_type):
    # ã€ä¿®æ”¹1ã€‘ç§»é™¤ .lower()ï¼Œä½¿å…³é”®è¯ä¿æŒåŸå§‹å¤§å°å†™
    keywords = [kw.strip() for kw in keywords_str.replace('\n', ',').replace(' ', ',').split(',') if kw.strip()]
    if not keywords:
        return nodes
    
    logging.info(f"    - æ‰§è¡Œ {filter_type} è¿‡æ»¤ (å¤§å°å†™æ•æ„Ÿ)ï¼Œå…³é”®è¯: {', '.join(keywords)}")
    
    original_count = len(nodes)
    
    # ã€ä¿®æ”¹2ã€‘ç§»é™¤ .lower()ï¼Œä½¿èŠ‚ç‚¹åç§°ä¹Ÿä¿æŒåŸå§‹å¤§å°å†™æ¥è¿›è¡ŒåŒ¹é…
    filtered_nodes = [node for node in nodes if not any(kw in node.get('name', '') for kw in keywords)]
    
    logging.info(f"      è¿‡æ»¤æ•ˆæœ: {original_count} -> {len(filtered_nodes)} (ç§»é™¤äº† {original_count - len(filtered_nodes)} ä¸ªèŠ‚ç‚¹)")
    
    return filtered_nodes
# --------------------------------------------------------------------------
#ã€â†‘â†‘â†‘ ä¸»è¦ä¿®æ”¹ç‚¹åœ¨è¿™é‡Œ â†‘â†‘â†‘ã€‘
# --------------------------------------------------------------------------

# =================================================================================
# ä¸»è·¯ç”±å’Œèšåˆé€»è¾‘
# ... (æ­¤éƒ¨åˆ†æ— æ”¹åŠ¨) ...
# =================================================================================

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/aggregate/clash.yaml')
def aggregate_clash():
    logging.info("="*30 + " [æ–°èšåˆè¯·æ±‚å¼€å§‹] " + "="*30)
    logging.info(f"æ¥æºIP: {request.remote_addr}, User-Agent: {request.user_agent.string}")
    
    data = load_data()
    enabled_ids = set(data.get('aggregation_enabled', []))
    subscriptions_to_aggregate = [sub for sub in data.get('subscriptions', []) if sub.get('id') in enabled_ids]
    
    logging.info(f"- æ‰¾åˆ° {len(data.get('subscriptions', []))} ä¸ªå·²å­˜è®¢é˜…ï¼Œå…¶ä¸­ {len(subscriptions_to_aggregate)} ä¸ªå·²å¯ç”¨èšåˆã€‚")

    if not subscriptions_to_aggregate:
        logging.warning("  - æ²¡æœ‰ä»»ä½•å¯ç”¨çš„è®¢é˜…ï¼Œæ— æ³•ç”Ÿæˆèšåˆé…ç½®ã€‚")
        return make_response("æ²¡æœ‰å¯ç”¨çš„è®¢é˜…ï¼Œæ— æ³•ç”Ÿæˆèšåˆé…ç½®ã€‚", 404)

    all_nodes = []
    headers = {'User-Agent': 'Clash/2023.08.17'}

    for sub_info in subscriptions_to_aggregate:
        sub_name = sub_info.get('name', 'Unnamed')
        sub_url = sub_info.get('url', '').strip()
        sub_filter_keywords = sub_info.get('filter_keywords', '')
        sub_filter_enabled = sub_info.get('filter_enabled', False)

        if not sub_url:
            logging.warning(f"  - è®¢é˜… '{sub_name}' URLä¸ºç©ºï¼Œå·²è·³è¿‡ã€‚")
            continue

        logging.info(f"  - æ­£åœ¨å¤„ç†è®¢é˜… '{sub_name}': {sub_url[:70]}...")

        try:
            sub_response = requests.get(sub_url, timeout=20, headers=headers)
            sub_response.raise_for_status() 
            logging.info(f"    - ä¸‹è½½æˆåŠŸ (çŠ¶æ€ç : {sub_response.status_code})")
            
            raw_content = sub_response.text
            nodes_from_sub = []
            
            try:
                clash_config = yaml.safe_load(raw_content)
                if 'proxies' in clash_config and isinstance(clash_config['proxies'], list):
                    nodes_from_sub = clash_config['proxies']
                    logging.info(f"    - è¯†åˆ«ä¸ºClashé…ç½®æ–‡ä»¶æ ¼å¼ï¼ŒæˆåŠŸæå– {len(nodes_from_sub)} ä¸ªèŠ‚ç‚¹ã€‚")
            except (yaml.YAMLError, AttributeError, TypeError):
                logging.info("    - æ— æ³•è§£æä¸ºYAMLï¼Œå°è¯•ä½œä¸ºé“¾æ¥åˆ—è¡¨å¤„ç†...")
                try: 
                    content = base64.b64decode(raw_content).decode('utf-8')
                    logging.info("      - Base64è§£ç æˆåŠŸã€‚")
                except (binascii.Error, UnicodeDecodeError): 
                    content = raw_content
                    logging.info("      - éBase64ç¼–ç ï¼Œä½œä¸ºçº¯æ–‡æœ¬å¤„ç†ã€‚")
                
                links = content.splitlines()
                logging.info(f"      - æ‰¾åˆ° {len(links)} è¡Œå†…å®¹ï¼Œå¼€å§‹é€è¡Œè§£æ...")
                for link in links:
                    if link.strip():
                        node = parse_link(link.strip())
                        if node: 
                            nodes_from_sub.append(node)
                logging.info(f"    - ä»é“¾æ¥åˆ—è¡¨æˆåŠŸè§£æå‡º {len(nodes_from_sub)} ä¸ªèŠ‚ç‚¹ã€‚")

            logging.info(f"    - ä¸º {len(nodes_from_sub)} ä¸ªèŠ‚ç‚¹æ·»åŠ å‰ç¼€ '[{sub_name}]'")
            for node in nodes_from_sub:
                if 'name' in node and not node['name'].startswith(f"[{sub_name}] "):
                    node['name'] = f"[{sub_name}] " + node['name']
            
            if sub_filter_enabled and sub_filter_keywords:
                nodes_after_sub_filter = apply_filter(nodes_from_sub, sub_filter_keywords, f"è®¢é˜…å†…[{sub_name}]")
                all_nodes.extend(nodes_after_sub_filter)
            else:
                logging.info("    - è®¢é˜…å†…è¿‡æ»¤å™¨æœªå¼€å¯ã€‚")
                all_nodes.extend(nodes_from_sub)

        except requests.RequestException as e:
            logging.error(f"  - [é”™è¯¯] ä¸‹è½½è®¢é˜… '{sub_name}' å¤±è´¥: {e}. å·²è·³è¿‡ã€‚")
            continue
        except Exception as e:
            logging.error(f"  - [é”™è¯¯] å¤„ç†è®¢é˜… '{sub_name}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=False)
            continue
    
    logging.info(f"- æ‰€æœ‰è®¢é˜…å¤„ç†å®Œæ¯•ï¼Œåˆå¹¶åå…± {len(all_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
    final_nodes = all_nodes
    
    global_filter_enabled = data.get('global_filter_enabled', False)
    global_filter_keywords = data.get('global_filter_keywords', '')
    if global_filter_enabled and global_filter_keywords:
        final_nodes = apply_filter(all_nodes, global_filter_keywords, "å…¨å±€")
    else:
        logging.info("- å…¨å±€è¿‡æ»¤å™¨æœªå¼€å¯ã€‚")
    
    logging.info(f"- æœ€ç»ˆèŠ‚ç‚¹æ•°: {len(final_nodes)} ä¸ªã€‚")
    
    if not final_nodes:
        logging.warning("  - æ‰€æœ‰èŠ‚ç‚¹å‡è¢«è¿‡æ»¤æˆ–è·å–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆé…ç½®ã€‚")
        return make_response("æ‰€æœ‰èŠ‚ç‚¹å‡è¢«è¿‡æ»¤æˆ–è·å–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆé…ç½®ã€‚", 400)
    
    try:
        template_path = CLASH_TEMPLATE_FILE
        if os.path.exists(template_path):
             logging.info(f"- æ‰¾åˆ°å¤–éƒ¨æ¨¡æ¿ '{template_path}'ï¼Œå°†ä½¿ç”¨å®ƒã€‚")
             with open(template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
        else:
            logging.error(f"  - [ä¸¥é‡é”™è¯¯] æœªæ‰¾åˆ° '{template_path}'ï¼Œæ— æ³•ç”Ÿæˆé…ç½®ã€‚")
            return make_response(f"é”™è¯¯ï¼šæ¨¡æ¿æ–‡ä»¶ '{template_path}' æœªæ‰¾åˆ°ã€‚", 500)
        
        final_config_str = generate_clash_config(final_nodes, template_content)
        
        response = make_response(final_config_str)
        response.headers['Content-Type'] = 'text/plain; charset=utf-8'
        response.headers['Content-Disposition'] = 'attachment; filename=clash.yaml'
        logging.info("="*31 + " [èšåˆè¯·æ±‚æˆåŠŸ] " + "="*31)
        return response

    except Exception as e:
        logging.critical(f"  - [ä¸¥é‡é”™è¯¯] ç”Ÿæˆæœ€ç»ˆé…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
        return make_response(f"æœåŠ¡å™¨é”™è¯¯: {e}", 500)

# =================================================================================
# API è·¯ç”±
# ... (æ­¤éƒ¨åˆ†æ— æ”¹åŠ¨) ...
# =================================================================================

@app.route('/api/data', methods=['GET'])
def get_data():
    logging.info("[API] GET /api/data - è¯·æ±‚æ‰€æœ‰æ•°æ®")
    return jsonify(load_data())

@app.route('/api/subscriptions', methods=['POST'])
def add_subscription():
    req_data = request.get_json()
    logging.info(f"[API] POST /api/subscriptions - æ·»åŠ æ–°è®¢é˜…: {req_data.get('name')}")
    if not req_data or not req_data.get('name') or not req_data.get('url'):
        logging.warning("[API] æ·»åŠ è®¢é˜…å¤±è´¥: è¯·æ±‚æ•°æ®ä¸å®Œæ•´")
        return jsonify({"message": "è¯·æ±‚æ•°æ®ä¸å®Œæ•´"}), 400
    
    data = load_data()
    new_sub = {
        "id": str(int(time.time() * 1000)), "name": req_data['name'], "url": req_data['url'],
        "filter_enabled": False, "filter_keywords": ""
    }
    data['subscriptions'].append(new_sub)
    save_data(data)
    logging.info(f"  > è®¢é˜… '{req_data.get('name')}' æ·»åŠ æˆåŠŸã€‚")
    return jsonify({"message": "è®¢é˜…æ·»åŠ æˆåŠŸ"}), 201

@app.route('/api/subscriptions/<sub_id>', methods=['PUT'])
def update_subscription(sub_id):
    req_data = request.get_json()
    logging.info(f"[API] PUT /api/subscriptions/{sub_id} - æ›´æ–°è®¢é˜…: {req_data.get('name')}")
    
    data = load_data()
    for sub in data['subscriptions']:
        if sub['id'] == sub_id:
            sub.update(req_data)
            save_data(data)
            logging.info(f"  > è®¢é˜… ID {sub_id} æ›´æ–°æˆåŠŸã€‚")
            return jsonify({"message": "è®¢é˜…æ›´æ–°æˆåŠŸ"})
            
    logging.warning(f"[API] æ›´æ–°è®¢é˜…å¤±è´¥: æœªæ‰¾åˆ°ID {sub_id}")
    return jsonify({"message": "æœªæ‰¾åˆ°è®¢é˜…"}), 404

@app.route('/api/subscriptions/<sub_id>', methods=['DELETE'])
def delete_subscription(sub_id):
    logging.info(f"[API] DELETE /api/subscriptions/{sub_id} - åˆ é™¤è®¢é˜…")
    data = load_data()
    original_len = len(data['subscriptions'])
    data['subscriptions'] = [sub for sub in data['subscriptions'] if sub['id'] != sub_id]
    
    if len(data['subscriptions']) < original_len:
        if 'aggregation_enabled' in data:
            data['aggregation_enabled'] = [id_ for id_ in data['aggregation_enabled'] if id_ != sub_id]
        save_data(data)
        logging.info(f"  > è®¢é˜… ID {sub_id} åˆ é™¤æˆåŠŸã€‚")
        return jsonify({"message": "è®¢é˜…åˆ é™¤æˆåŠŸ"})
    
    logging.warning(f"[API] åˆ é™¤è®¢é˜…å¤±è´¥: æœªæ‰¾åˆ°ID {sub_id}")
    return jsonify({"message": "æœªæ‰¾åˆ°è®¢é˜…"}), 404

@app.route('/api/aggregation', methods=['POST'])
def save_aggregation_settings():
    req_data = request.get_json()
    enabled_count = len(req_data.get('enabled_ids', []))
    logging.info(f"[API] POST /api/aggregation - ä¿å­˜èšåˆé€‰æ‹©ï¼Œå¯ç”¨ {enabled_count} ä¸ªè®¢é˜…")
    
    data = load_data()
    data['aggregation_enabled'] = req_data.get('enabled_ids', [])
    save_data(data)
    return jsonify({"message": "èšåˆé€‰æ‹©å·²ä¿å­˜"})

@app.route('/api/global_filter', methods=['POST'])
def save_global_filter():
    req_data = request.get_json()
    is_enabled = req_data.get('enabled', False)
    logging.info(f"[API] POST /api/global_filter - ä¿å­˜å…¨å±€è¿‡æ»¤å™¨ï¼ŒçŠ¶æ€: {'å¯ç”¨' if is_enabled else 'ç¦ç”¨'}")

    data = load_data()
    data['global_filter_enabled'] = is_enabled
    data['global_filter_keywords'] = req_data.get('keywords', '')
    save_data(data)
    return jsonify({"message": "å…¨å±€è¿‡æ»¤å™¨å·²ä¿å­˜"})

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    logging.info("=" * 20 + " Sub Aggregator åº”ç”¨å¯åŠ¨ " + "=" * 20)
    app.run(host='0.0.0.0', port=5000, debug=False)

